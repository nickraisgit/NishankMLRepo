{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding out the most accurate model to predict a malignant tumor using mammogram data\n",
    "\n",
    "This project is aimed to figure out what the most accurate and effective model is for predictive modeling. I am going to use a decision tree classifier, a random forest classifier, a XGBoost classifier, K nearest neighbors method, Naive Bayes, Support Vector Machines, and finally a nueral network run on Keras. We will be finding the accuracy using K fold cross validation where it can be applied, otherwise we will use the provided accuracy score methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important part of any data science is actually working on cleaning your data. New frameworks make it really easy to actually implement your algorithms, but the data cleaning is still based on us. We need to decide which data we actually need, which data to drop, which data is relevent and whether there is some implicit bias in your data. All these things need to be factored in so we can actually make our model work effeciently. Remember, the model won't do everything for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BI_RADS</th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BI_RADS   age  shape  margin  density  severity\n",
       "0      5.0  67.0    3.0     5.0      3.0         1\n",
       "1      4.0  43.0    1.0     1.0      NaN         1\n",
       "2      5.0  58.0    4.0     5.0      3.0         1\n",
       "3      4.0  28.0    1.0     1.0      3.0         0\n",
       "4      5.0  74.0    1.0     5.0      NaN         1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Let's Load in our data\n",
    "feature_names = ['BI_RADS', 'age', 'shape', 'margin', 'density', 'severity']\n",
    "df = pd.read_csv(\"mammographic_masses.data.txt\", na_values = ['?'], names = feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BI_RADS</th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>959.000000</td>\n",
       "      <td>956.000000</td>\n",
       "      <td>930.000000</td>\n",
       "      <td>913.000000</td>\n",
       "      <td>885.000000</td>\n",
       "      <td>961.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.348279</td>\n",
       "      <td>55.487448</td>\n",
       "      <td>2.721505</td>\n",
       "      <td>2.796276</td>\n",
       "      <td>2.910734</td>\n",
       "      <td>0.463059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.783031</td>\n",
       "      <td>14.480131</td>\n",
       "      <td>1.242792</td>\n",
       "      <td>1.566546</td>\n",
       "      <td>0.380444</td>\n",
       "      <td>0.498893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          BI_RADS         age       shape      margin     density    severity\n",
       "count  959.000000  956.000000  930.000000  913.000000  885.000000  961.000000\n",
       "mean     4.348279   55.487448    2.721505    2.796276    2.910734    0.463059\n",
       "std      1.783031   14.480131    1.242792    1.566546    0.380444    0.498893\n",
       "min      0.000000   18.000000    1.000000    1.000000    1.000000    0.000000\n",
       "25%      4.000000   45.000000    2.000000    1.000000    3.000000    0.000000\n",
       "50%      4.000000   57.000000    3.000000    3.000000    3.000000    0.000000\n",
       "75%      5.000000   66.000000    4.000000    4.000000    3.000000    1.000000\n",
       "max     55.000000   96.000000    4.000000    5.000000    4.000000    1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BI_RADS</th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>830.000000</td>\n",
       "      <td>830.000000</td>\n",
       "      <td>830.000000</td>\n",
       "      <td>830.000000</td>\n",
       "      <td>830.000000</td>\n",
       "      <td>830.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.393976</td>\n",
       "      <td>55.781928</td>\n",
       "      <td>2.781928</td>\n",
       "      <td>2.813253</td>\n",
       "      <td>2.915663</td>\n",
       "      <td>0.485542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.888371</td>\n",
       "      <td>14.671782</td>\n",
       "      <td>1.242361</td>\n",
       "      <td>1.567175</td>\n",
       "      <td>0.350936</td>\n",
       "      <td>0.500092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          BI_RADS         age       shape      margin     density    severity\n",
       "count  830.000000  830.000000  830.000000  830.000000  830.000000  830.000000\n",
       "mean     4.393976   55.781928    2.781928    2.813253    2.915663    0.485542\n",
       "std      1.888371   14.671782    1.242361    1.567175    0.350936    0.500092\n",
       "min      0.000000   18.000000    1.000000    1.000000    1.000000    0.000000\n",
       "25%      4.000000   46.000000    2.000000    1.000000    3.000000    0.000000\n",
       "50%      4.000000   57.000000    3.000000    3.000000    3.000000    0.000000\n",
       "75%      5.000000   66.000000    4.000000    4.000000    3.000000    1.000000\n",
       "max     55.000000   96.000000    4.000000    5.000000    4.000000    1.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop all the rows with Null values. It seems like the null values are randomly distributed, so we aren't adding \n",
    "#Any bias to our model\n",
    "df.dropna(inplace=True)\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BI_RADS</th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>815.000000</td>\n",
       "      <td>815.000000</td>\n",
       "      <td>815.000000</td>\n",
       "      <td>815.000000</td>\n",
       "      <td>815.000000</td>\n",
       "      <td>815.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.341104</td>\n",
       "      <td>55.694479</td>\n",
       "      <td>2.770552</td>\n",
       "      <td>2.801227</td>\n",
       "      <td>2.915337</td>\n",
       "      <td>0.480982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.579304</td>\n",
       "      <td>14.695890</td>\n",
       "      <td>1.244197</td>\n",
       "      <td>1.570536</td>\n",
       "      <td>0.352524</td>\n",
       "      <td>0.499945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          BI_RADS         age       shape      margin     density    severity\n",
       "count  815.000000  815.000000  815.000000  815.000000  815.000000  815.000000\n",
       "mean     4.341104   55.694479    2.770552    2.801227    2.915337    0.480982\n",
       "std      0.579304   14.695890    1.244197    1.570536    0.352524    0.499945\n",
       "min      2.000000   18.000000    1.000000    1.000000    1.000000    0.000000\n",
       "25%      4.000000   45.500000    2.000000    1.000000    3.000000    0.000000\n",
       "50%      4.000000   57.000000    3.000000    3.000000    3.000000    0.000000\n",
       "75%      5.000000   66.000000    4.000000    4.000000    3.000000    1.000000\n",
       "max      5.000000   96.000000    4.000000    5.000000    4.000000    1.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now lets filter for outliers. If we see the description of our data we can see that the maximum BI_RADS value is 5\n",
    "#While it is on a scale of 1-5. So we can already tell this is a bit fishy, so we need to take out the outlier\n",
    "#So our data doesn't get skewed in any way.\n",
    "df_filtered = df[df['BI_RADS'] < 6]\n",
    "df_filtered = df_filtered[df_filtered['BI_RADS']>0]\n",
    "df_filtered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'shape', 'margin', 'density']\n"
     ]
    }
   ],
   "source": [
    "#Now we need to define our features. We are using age, shape, margin, and density of the tumor because BI_RADS is not a predective stat\n",
    "scaler = StandardScaler()\n",
    "features = list(df.columns[[1,2,3,4]])\n",
    "print(features)\n",
    "labels = df['severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = scaler.fit(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BI_RADS</th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>830.000000</td>\n",
       "      <td>8.300000e+02</td>\n",
       "      <td>8.300000e+02</td>\n",
       "      <td>8.300000e+02</td>\n",
       "      <td>8.300000e+02</td>\n",
       "      <td>830.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.393976</td>\n",
       "      <td>2.635108e-17</td>\n",
       "      <td>-1.958273e-16</td>\n",
       "      <td>2.037694e-16</td>\n",
       "      <td>1.180448e-16</td>\n",
       "      <td>0.485542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.888371</td>\n",
       "      <td>1.000603e+00</td>\n",
       "      <td>1.000603e+00</td>\n",
       "      <td>1.000603e+00</td>\n",
       "      <td>1.000603e+00</td>\n",
       "      <td>0.500092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.576695e+00</td>\n",
       "      <td>-1.435172e+00</td>\n",
       "      <td>-1.157718e+00</td>\n",
       "      <td>-5.462015e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>-6.671191e-01</td>\n",
       "      <td>-6.297680e-01</td>\n",
       "      <td>-1.157718e+00</td>\n",
       "      <td>2.404661e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.307148e-02</td>\n",
       "      <td>1.756364e-01</td>\n",
       "      <td>1.192334e-01</td>\n",
       "      <td>2.404661e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.968638e-01</td>\n",
       "      <td>9.810408e-01</td>\n",
       "      <td>7.577091e-01</td>\n",
       "      <td>2.404661e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>2.742838e+00</td>\n",
       "      <td>9.810408e-01</td>\n",
       "      <td>1.396185e+00</td>\n",
       "      <td>3.091707e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          BI_RADS           age         shape        margin       density  \\\n",
       "count  830.000000  8.300000e+02  8.300000e+02  8.300000e+02  8.300000e+02   \n",
       "mean     4.393976  2.635108e-17 -1.958273e-16  2.037694e-16  1.180448e-16   \n",
       "std      1.888371  1.000603e+00  1.000603e+00  1.000603e+00  1.000603e+00   \n",
       "min      0.000000 -2.576695e+00 -1.435172e+00 -1.157718e+00 -5.462015e+00   \n",
       "25%      4.000000 -6.671191e-01 -6.297680e-01 -1.157718e+00  2.404661e-01   \n",
       "50%      4.000000  8.307148e-02  1.756364e-01  1.192334e-01  2.404661e-01   \n",
       "75%      5.000000  6.968638e-01  9.810408e-01  7.577091e-01  2.404661e-01   \n",
       "max     55.000000  2.742838e+00  9.810408e-01  1.396185e+00  3.091707e+00   \n",
       "\n",
       "         severity  \n",
       "count  830.000000  \n",
       "mean     0.485542  \n",
       "std      0.500092  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      0.000000  \n",
       "75%      1.000000  \n",
       "max      1.000000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we scale our data down using a standard scaler function, this makes the data more usable by the model and keeps it normalized.\n",
    "feats = df[features]\n",
    "df[features] = scaler.transform(df[features])\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df[features], labels, train_size = 0.75, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52380952 0.71428571 0.71428571 0.76190476 0.71428571 0.71428571\n",
      " 0.66666667 0.66666667 0.8        0.7       ]\n",
      "0.6976190476190476\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, x_test, y_test, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see here the decision tree is not that accurate, but there are still a lot of methods left to try so lets keep going!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf2 = RandomForestClassifier(n_estimators=10)\n",
    "clf2 = clf2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61904762 0.71428571 0.61904762 0.85714286 0.76190476 0.66666667\n",
      " 0.61904762 0.71428571 0.85       0.8       ]\n",
      "0.7221428571428571\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf2, x_test, y_test, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not a very high accuracy, but still better than the regular decision tree. Of course 73% accuracy isn't good enough in something as important as cancer detection so let's go on to the next method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth': 4,\n",
    "    'eta': 0.01,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class':2} \n",
    "epochs = 100\n",
    "train = xgb.DMatrix(x_train, label=y_train)\n",
    "test = xgb.DMatrix(x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:17:10] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgbmodel = xgb.train(param, train, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7788461538461539"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = xgbmodel.predict(test)\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so as we keep on going we can see that this is at least an upwards trend in the accuracy of the model, so lets move on to other techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors =6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Nearest Neighbors is a method that basically puts all the parameters on a graph and uses the average of the K(some integer) number of points closest to point you want to test. This is a very simple method, but a very effective one, if you can get the sweetspot of the K value. In this case that \"sweetspot\" is 6 neighbors, you can try different amounts to see the difference in accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = knn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66666667 0.71428571 0.71428571 0.85714286 0.80952381 0.80952381\n",
      " 0.80952381 0.80952381 0.75       0.75      ]\n",
      "0.7690476190476191\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(knn, x_test, y_test, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is still getting better but not quite at the level we want it yet. We are aiming for at least 85% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to rescale our parameters to a min max format due to the fact that the Naive Bayes Classifier does not take negative inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scale = MinMaxScaler()\n",
    "scale = scale.fit(x_train)\n",
    "x_train2 = scale.transform(x_train)\n",
    "classifier = MultinomialNB()\n",
    "counts = (x_train2)\n",
    "targets = y_train.values\n",
    "classifier.fit(counts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76190476 0.76190476 0.71428571 0.95238095 0.80952381 0.80952381\n",
      " 0.71428571 0.80952381 0.7        0.85      ]\n",
      "0.7883333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_test2 = scale.transform(x_test)\n",
    "scores = cross_val_score(classifier, x_test2, y_test.values, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see again that it is defenitely getting more accurate but it is still staying around that 75-78% range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "C = 1.0\n",
    "svc = svm.SVC(kernel='rbf', C=C).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can play around with the different types of kernels but in this case the rbf and polynomial kernel seem to work the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71428571 0.80952381 0.76190476 0.95238095 0.80952381 0.80952381\n",
      " 0.71428571 0.80952381 0.75       0.85      ]\n",
      "0.7980952380952381\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(svc, x_test2, y_test.values, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still sort of in that 78-80% accuracy range and we are sort of in a stalemate here. So now let's try out a deep learning neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this neural network we are going to use the Keras API which works on top of the Tensorflow framework. Keras makes it really easy to do this, and makes calculating the gradient descent very easy. We are going to be using a layer of 64 hidden neurons to go along with 128 input neurons. We are going to run 10 epochs and dropout 20% of each of our neurons to prevent overfitting of our model. We will also be incorporating sklearn when we run the model, to see how it can play a part in deep learning and also to use K Cross validation. We will be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/42\n",
      "9/9 - 4s - loss: 0.6449 - accuracy: 0.6945 - val_loss: 0.5616 - val_accuracy: 0.7885\n",
      "Epoch 2/42\n",
      "9/9 - 0s - loss: 0.5123 - accuracy: 0.7990 - val_loss: 0.4728 - val_accuracy: 0.7788\n",
      "Epoch 3/42\n",
      "9/9 - 0s - loss: 0.4748 - accuracy: 0.8039 - val_loss: 0.4843 - val_accuracy: 0.7740\n",
      "Epoch 4/42\n",
      "9/9 - 0s - loss: 0.4612 - accuracy: 0.7878 - val_loss: 0.4685 - val_accuracy: 0.7788\n",
      "Epoch 5/42\n",
      "9/9 - 0s - loss: 0.4621 - accuracy: 0.7830 - val_loss: 0.4677 - val_accuracy: 0.7837\n",
      "Epoch 6/42\n",
      "9/9 - 0s - loss: 0.4670 - accuracy: 0.7846 - val_loss: 0.4731 - val_accuracy: 0.7837\n",
      "Epoch 7/42\n",
      "9/9 - 0s - loss: 0.4548 - accuracy: 0.7926 - val_loss: 0.4800 - val_accuracy: 0.7837\n",
      "Epoch 8/42\n",
      "9/9 - 0s - loss: 0.4474 - accuracy: 0.8087 - val_loss: 0.4723 - val_accuracy: 0.7981\n",
      "Epoch 9/42\n",
      "9/9 - 0s - loss: 0.4556 - accuracy: 0.7958 - val_loss: 0.4738 - val_accuracy: 0.7885\n",
      "Epoch 10/42\n",
      "9/9 - 0s - loss: 0.4697 - accuracy: 0.7942 - val_loss: 0.4734 - val_accuracy: 0.7837\n",
      "Epoch 11/42\n",
      "9/9 - 0s - loss: 0.4685 - accuracy: 0.7846 - val_loss: 0.4684 - val_accuracy: 0.7933\n",
      "Epoch 12/42\n",
      "9/9 - 0s - loss: 0.4735 - accuracy: 0.7894 - val_loss: 0.4670 - val_accuracy: 0.7885\n",
      "Epoch 13/42\n",
      "9/9 - 0s - loss: 0.4550 - accuracy: 0.7910 - val_loss: 0.4663 - val_accuracy: 0.7885\n",
      "Epoch 14/42\n",
      "9/9 - 0s - loss: 0.4584 - accuracy: 0.7974 - val_loss: 0.4744 - val_accuracy: 0.7933\n",
      "Epoch 15/42\n",
      "9/9 - 0s - loss: 0.4591 - accuracy: 0.7910 - val_loss: 0.4719 - val_accuracy: 0.7981\n",
      "Epoch 16/42\n",
      "9/9 - 0s - loss: 0.4575 - accuracy: 0.7990 - val_loss: 0.4710 - val_accuracy: 0.7933\n",
      "Epoch 17/42\n",
      "9/9 - 0s - loss: 0.4668 - accuracy: 0.7942 - val_loss: 0.4686 - val_accuracy: 0.7981\n",
      "Epoch 18/42\n",
      "9/9 - 0s - loss: 0.4483 - accuracy: 0.8119 - val_loss: 0.4732 - val_accuracy: 0.7933\n",
      "Epoch 19/42\n",
      "9/9 - 0s - loss: 0.4767 - accuracy: 0.7814 - val_loss: 0.4687 - val_accuracy: 0.8029\n",
      "Epoch 20/42\n",
      "9/9 - 0s - loss: 0.4685 - accuracy: 0.7942 - val_loss: 0.4685 - val_accuracy: 0.7981\n",
      "Epoch 21/42\n",
      "9/9 - 0s - loss: 0.4505 - accuracy: 0.7958 - val_loss: 0.4697 - val_accuracy: 0.7885\n",
      "Epoch 22/42\n",
      "9/9 - 0s - loss: 0.4415 - accuracy: 0.8023 - val_loss: 0.4740 - val_accuracy: 0.7933\n",
      "Epoch 23/42\n",
      "9/9 - 0s - loss: 0.4704 - accuracy: 0.7862 - val_loss: 0.4701 - val_accuracy: 0.8029\n",
      "Epoch 24/42\n",
      "9/9 - 0s - loss: 0.4428 - accuracy: 0.8119 - val_loss: 0.4706 - val_accuracy: 0.8029\n",
      "Epoch 25/42\n",
      "9/9 - 0s - loss: 0.4500 - accuracy: 0.7990 - val_loss: 0.4765 - val_accuracy: 0.7981\n",
      "Epoch 26/42\n",
      "9/9 - 0s - loss: 0.4632 - accuracy: 0.7846 - val_loss: 0.4691 - val_accuracy: 0.8029\n",
      "Epoch 27/42\n",
      "9/9 - 0s - loss: 0.4504 - accuracy: 0.7958 - val_loss: 0.4723 - val_accuracy: 0.7933\n",
      "Epoch 28/42\n",
      "9/9 - 0s - loss: 0.4546 - accuracy: 0.7974 - val_loss: 0.4792 - val_accuracy: 0.7788\n",
      "Epoch 29/42\n",
      "9/9 - 0s - loss: 0.4384 - accuracy: 0.8055 - val_loss: 0.4719 - val_accuracy: 0.7885\n",
      "Epoch 30/42\n",
      "9/9 - 0s - loss: 0.4551 - accuracy: 0.7942 - val_loss: 0.4729 - val_accuracy: 0.7933\n",
      "Epoch 31/42\n",
      "9/9 - 0s - loss: 0.4538 - accuracy: 0.8119 - val_loss: 0.4746 - val_accuracy: 0.7885\n",
      "Epoch 32/42\n",
      "9/9 - 0s - loss: 0.4631 - accuracy: 0.7942 - val_loss: 0.4710 - val_accuracy: 0.7981\n",
      "Epoch 33/42\n",
      "9/9 - 0s - loss: 0.4530 - accuracy: 0.8006 - val_loss: 0.4744 - val_accuracy: 0.7981\n",
      "Epoch 34/42\n",
      "9/9 - 0s - loss: 0.4568 - accuracy: 0.7974 - val_loss: 0.4771 - val_accuracy: 0.7933\n",
      "Epoch 35/42\n",
      "9/9 - 0s - loss: 0.4558 - accuracy: 0.8006 - val_loss: 0.4723 - val_accuracy: 0.8077\n",
      "Epoch 36/42\n",
      "9/9 - 0s - loss: 0.4564 - accuracy: 0.7942 - val_loss: 0.4750 - val_accuracy: 0.7933\n",
      "Epoch 37/42\n",
      "9/9 - 0s - loss: 0.4494 - accuracy: 0.7974 - val_loss: 0.4753 - val_accuracy: 0.7981\n",
      "Epoch 38/42\n",
      "9/9 - 0s - loss: 0.4633 - accuracy: 0.7894 - val_loss: 0.4757 - val_accuracy: 0.8029\n",
      "Epoch 39/42\n",
      "9/9 - 0s - loss: 0.4532 - accuracy: 0.7942 - val_loss: 0.4750 - val_accuracy: 0.7933\n",
      "Epoch 40/42\n",
      "9/9 - 0s - loss: 0.4334 - accuracy: 0.8055 - val_loss: 0.4737 - val_accuracy: 0.7885\n",
      "Epoch 41/42\n",
      "9/9 - 0s - loss: 0.4552 - accuracy: 0.8006 - val_loss: 0.4734 - val_accuracy: 0.7933\n",
      "Epoch 42/42\n",
      "9/9 - 0s - loss: 0.4383 - accuracy: 0.8006 - val_loss: 0.4711 - val_accuracy: 0.7981\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, input_dim=4, kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal',activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "model = create_model()\n",
    "history = model.fit(x_train, y_train, batch_size=75, epochs=42, verbose=2, validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.47111397981643677\n",
      "Test accuracy: 0.7980769276618958\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss: \", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see again that the accuracy is actually very close to what it was before, so the neural network doesn't really make a big difference compared to most of the other tedchniques that we used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So which method is the best??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is actually no clear best method here. There is a clear worst method though, and that is a basic decision tree. It makes sense, high dimension data doesn't really work all that well with a basic decision tree compared to the other methods, like xgboost, a random forest, or a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basically anything except for a basic decision tree works well, which is going to be the case in most scenarios"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
